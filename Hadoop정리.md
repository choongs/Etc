# Hadoop

## 리눅스 파일시스템

File : 데이터나 프로그램을 담는 그릇.
File System : 파일을 관리하는 시스템
메타데이터 : 파일을 관리하는 정보
- 파일의 이름, 확장자, 권한의 정보 등..

디렉토리 : 파일을 관리하기위한 도구.
- 파일시스템 관저에서는 디렉토리도 하나의 파일이라고 생각함.
- Tree 구조
- Directory Entry : 디렉토리를 표현하는데 쓰이는 자료구조, 유닉스계열에는 파일이름과 , Inode를 가르키는 포인트정보만 가지고있다.
- Inode : 파일이름을 제외한 메타데이터 정보를 가지고 있다.


Link (바로가기 같은 느낌)
- 소프트링크 (심볼릭 링크): 원본파일을 가르키는 포인트를 가지고있음 
- 하드링크 : 원본파일을 복사함, 하지만 리눅에선 사용못하게 막았음. 순환참조 오류문제.

권한 
소유자(user), 그룹(group), 그외(others)로 구분해서 줄수있음.
권한은 총 3가지 읽기, 쓰기, 실행
권한을 주는 방법은 2가지
1. 개별적으로 파일 권한을 주는방법
user : u, group : g, others : o
+: 추가, -: 빼기 ,= 할당
ex) u+x,g=rw,o-w (user에게 실행권한을 추가, 그룹에는 읽기쓰기권한을 할당 그외에는 쓰기권한을 회수) 
2. 숫자로 한번에주기
읽기는 4 쓰기는 2 실행은 1 숫자를 더해서 권한을 부여 
ex) chmod 744 text.txt (여기서 세자리수의 첫번째는 user, 두번째는 group, 마지막은 other)
즉 사용자에겐 모든 권한을 주고 그룹과 기타에는 읽기권한만줌


파일할당 
파일 할당을 하는 방법은 연속적인 할당, 연결할당, 색인할당이 있음
- 연속적인 할당은 데이터 블록을 연속적으로 사용해서 파일을 할당하는 방법, 해당방법은 파일의 크기가 커지나 작아지므로 인한 문제가 있음.
- 연결할당은 쉽게 말해 파일의 블록끼리 서로간의 연결이 되어있는 구조 첫번째 블록이 두번째 블록의 포인터를 가지고있어서 순서대로 연결되어있음.
- 리눅스는 색인할당을 사용하고있음, 색인할당은 연결할당과 비슷하게 제약없이 블록이 흩어져서 할당되어 있지만 연결할당과 다르게 색인된 포인터를 가지고있는 색인블록이 있어서 그 블록을 통해 파일을 컨트롤하는 구조.

## 분산 파일시스템
분산 파일 시스템이란 네트워크를 이용해 접근하는 파일시스템을 칭한다. 분산 파일 시스템의 종류로는 NFS(Network File System, CIFS(Common Internet File System, HDFS(Hadoop Distributed File System)가 있다.

우리가 흔히 알고 있는 NAS(Network Attached Storage)는 NFS, CIFS에 속한다.

#### NFS, CIFS
NFS: Linux/Unix 환경에서 사용 
CIFS: Winodows 환경에서 사용
- POSIX 표준을 준수하기때문에 로컬파일시스템처럼 사용할 수 있다.
- 비싸다..
- 확장성이 떨어진다.

#### HDFS 

구글은 웹페이지 정보를 크롤링해서 저장할 수 있는 GFS(Google File System)을 만들었고 이에 대한 논문을 공개했음, 이 논물을 바탕으로 HDFS라는 오픈소스가 만들어졌음

##### 하둡 특성
- N대 이상의 리눅스기반 범용 서버들을 하나의 클러스터로 사용.
- 마스터 - 슬레이브 구조
- 파일을 블록 단위로 저장
- 하나의 블록 파일은 기본 3개의 복제본을 가지고 있어서 신뢰성을 보장.
- 높은 내고장성 (Falut Tolerance)
- 데이터 처리의 지역성 보장 (Data Locality)


##### Master - Slave
HDFS는 하나의 마스터와 N대의 슬레이브로 구성되어진다. 여기서 중요한점은 마스터에는 불필요한 부하를 발생하지 않게하는게 중요하다. 그래서 모든 데이터 처리 및 I/O 작업은 클라이언트와 슬레이브끼리 이루어진다.
또한 슬레이브 서버는 N대의 서버들로 이루어져서 계속적인 확장을 이룰수있다.(Scale Out)

마스터노드 
- Name Node: 하둡분산파일시스템의 마스터 역할을 담당하고 슬레이브 역할을 담당하는 데이터노드에 I/O 작업을 지시 및 관리한다.

슬레이브노드
- Data Node : 데이터를 읽고 쓰는 역할

##### NameNode
- 전체 HDFS에 대한 메타데이터 관리
- 데이터 노드로 부터 블록에 대한 정보를 받음
- 데이터에 대한 Replication을 유지 관리.
- 파일시스템 이미지 파일관리(fsimage)
- 파일시스템에 대한 Edit Log관리

##### Secondary NameNode
NameNode의 이중화 역할이 아님.. 그냥 도와주는 역할
NameNode가 기동하게되면 fsimage(스냅샷)를 이용하여 메타정보를 메모리에 반영한다 그리고 edits log를 이용해서 변경된 내역을 반영하게된다. 그 후로 서비스를 시작하게되면서 파일 시스템의 변경된 내역은 edits log에 기록된다. 이렇게 변경된 내역들을 지속적으로 (1시간마다, 혹은 edits log 일정 사이즈이상) fsimage에 병합을하고 fsimage를 교체하는 역할을 Secondary namenode가 맡고있다.

Secondary NameNode가 장애가 발생해서 동작을 하지 않아도 그시점에는 문제가 발생하지 않는다. 단, 이렇게 되면 Edits log의 크기가 커지게 되고, 추후 네임노도를 재시작하면서 Edits log를 읽다가 Out of memory가 발생 가능성이 있다.

##### DataNode
- 물리적으로 로컬파일시스템에 HDFS데이터를 저장
- 일반적으로 레이드 구성을 하지않음
- 블록에 대한정보를 NameNode가 시작할때랑 주기적으로 HDFS블록리스트를 검사후 블록의 대한 데이터를 NameNode에게 보냄

##### 블록(Block)
- 하나의 파일을 여러개의 블록으로 저장
- 블록의 크기는 128MB(기본, 조정이 가능)
- 실제 파일이 128MB보다 작은 경우 실제 크기만큼의 블록이 만들어짐
- 블록의 크기가 큰 이유는 네임노드, 데이터노드, 태스크트래커에서 블록을 빨리 찾기위해서 메타정보를 줄이기위한 용도.

##### 파일의 저장
데이터노드에 파일을 저장하면 데이터를 블록단위(기본 128MB)로 조각내어 분산 저장하고 데이터 노드 스스로 다른 서버에 불록을 복제한다 (기분 3카피), 즉 이말은 동일한 서버내에서 동일한 블록이 복사되지 않고 다른서버 3대에서 동일한 블록이 유지되어진다.

##### 데이터노드 자동복구
기본적으로 데이터노드는 3초마다 Name Node에게 heartbeat을 보내게되어있다. 만약 일정시간 동안 heartbeat이 오지 않는다면 NameNode는 장애로 인식하고 해당 노드에 속한 복제된 블록중 데이터노드 한곳에 다른 서버에 블록을 복사하라고 명령을 보내서 기본 3카피 블록이 유지될수 있도록한다.


## 맵리듀스 (MapReduce)
맵리듀스는 구글에서 대용량 데이터 처리를 분산 병렬컴퓨팅에서 처리하기 위한 목적으로 제작하 소프트웨어 프레임워크이다. - 위키백과

##### 맵리듀스 알고리즘
- Map Function :(key1, value1) -> (key2, value2)
- Reduce Function : (key2, List of value2) -> (key3, value3)

##### 맵리듀스 1 구동방식 

1. 클라이언트에서 JobTracker로 Job을 서브밋(jar)
2. JobTracker는 job을 hdfs에서 저장 (모든 데이터노드에서 접근가능)
3. 공유되어있는 Job을 TaskTracker가 Local로 복사
4. TaskTraker는 child JVM을 통해 Job을 실행

- JobTracker : 맵리듀스 잡이 수행되는 전체 과정을 조정하며, 잡에 대한 마스 역할 수행
- TaskTracker : 잡에 대한 불한된 태스크를 수행 하며, 실질적인 데이타 처리 주체 (슬레이브)


##### 맵리듀스 단계
Input - Mapper - Combiner - Partitioner - Shuffle/sort Reducer - Output
######  Map Task
- Input : 입력데이터, 주로 텍스트 파일을 처리하지만, 이미지, 바이너리 파일 등 다양하다. (기본적으로 하둡에서 파일포맷들을 제공하고 커스텀 파일포맷도 만들수 있다.)
- Mapper : 전달받은 데이터를 처리하는 부분, 개행문자기준 한줄씩 읽어들여서 입력데이터를 원하는 key-value 형태로 만드는 작업
- Combiner : Combiner의 목적은 트랙픽을 줄이기위한 목적이 있다. 매퍼에서 넘겨온 값들을 다시 key 기준으로 합치는 작업을 통해서 리듀서에게 보내지는 데이터 전송을 줄일수 있다.
- Partitioner : 매퍼에서 나온 데이터들이 어느 리듀스 태스크로 가야할지 정해주는 역할을한다. key-value 구조의 데이터에서 key의 해시코드값과 리듀스 태스트의 갯수를 나누기 연산을 해서 그 나머지를 통해서 어느 리듀스태스로 보낼지 정한다.


###### Reduce Task
- Supple/sort : 리듀서에게 보내지는 일련과정들을 셔플이라고 한다. (파이셔너도 셔플의 포함됨) 리듀서가 데이터를 잘 받았다면 작업전에 전처리 작업으로 key값을 기준으로 정렬한다. 그 이유는 작업에 더 효율적으로 하기 위해서다.
- Reducer : 매퍼의 데이터를 입력 받아서 같은 키를 가지고 있는 데이터를 처리하는 작업
- Output : 리듀서가 모든작업을 끝나면 결과물을 파일로 출력한다.

##### 맵리듀스 진행상황과 상태체크 
태스크 트래커는 진행상황을 3초마다 잡트래커에게 전송을 하게 되어있고, 수행 도중에 특정 잡에대해서 장애가 발생하면 잡트래커는 동일한 데이터를 가지고있는 다른 태스크트래커에게 잡을 수행하라는 명령을 내려서 잡을 정상화함.

#### YARN
하둡 2.0에서 추가된 기술, 기존 1.0에서는 리소스 관리미 잡의 대한 관리를 잡트래커에서 모두 수행했지만, 2.0에서는 클러스터 리소스 매니지먼트와 데이터 처리 영역을 분리함.

##### YARN 동작흐름
1. 클라이언트가 Resource Manager에게 Appllication 서브밋.
2. Resource Manager는 전체 클러스터의 상태를 확인한 후 특정 Node Manager에게 잡명령을 내림
3. Node Manager는 Apllication Master를 구동(평소에는 떠있지 않음)
4. Apllication Master는 컨테이너에 Job수행을 지시.

- Resource Manager : 클라이언트가 요청한 어플리케이션 마다 자원을 관리 
- Node Manager : 각 슬레이브 보드 마다 1개 (논리적으로 1개), 컨테이너 와 자원의 상태를 Resource Manager에게 통지
- Application Manager : Application을 실행하고 관리하고 상태를 RM에게 통지 (하둡 1.0에서 잡트래커 역할)
- Contatiner : Application을 수행하고 상태를 Application Manager에게 통지 (하둡 1.0에서 태스트트래커 역할)




